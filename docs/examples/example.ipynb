{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIxChem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading\n",
    "The ```Dataset()``` class is one of the central objects in the framework. It offers a convenient way to handle X and y data simultanously.\n",
    "You can initiate a ```Dataset()``` instance from various sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from aixchem import Dataset\n",
    "\n",
    "# Using np arrays.\n",
    "data = np.random.rand(10, 4)\n",
    "target = np.random.rand(10, 1)\n",
    "dataset = Dataset(data, target)\n",
    "\n",
    "# Using pd dataframes and target from different source\n",
    "data = pd.DataFrame(np.random.rand(10, 4))\n",
    "target = pd.Series(np.random.rand(10))\n",
    "dataset = Dataset(data, target)\n",
    "\n",
    "# Using pd dataframes and target from the same source\n",
    "data = pd.DataFrame(np.random.rand(10, 4), columns=[\"A\", \"B\", \"C\", \"D\"])\n",
    "target = \"D\"\n",
    "dataset = Dataset(data, target)\n",
    "\n",
    "# Using pd dataframes and multiple targets from the same source.\n",
    "target = [\"D\", \"C\"]\n",
    "dataset = Dataset(data, target)\n",
    "\n",
    "# Loading from file.\n",
    "AIxChem = Path.cwd().parents[1]\n",
    "data = AIxChem / \"datasets\" / \"buttar_norrby_dataset.csv\"  # Adjust the path to where the results are located\n",
    "dataset = Dataset(data, target=\"exp_activation_energy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning\n",
    "The ```Dataset()``` class offers several functions for cleaning the data, dropping columns and rows etc. (see source code for more info). Some simple cleaning operations could look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aixchem import Dataset\n",
    "\n",
    "# Loading from file.\n",
    "AIxChem = Path.cwd().parents[1]\n",
    "data = AIxChem / \"datasets\" / \"buttar_norrby_dataset.csv\"  # Adjust the path to where the results are located\n",
    "dataset = Dataset(data, target=\"exp_activation_energy\")\n",
    "\n",
    "# Drop rows with missing values\n",
    "dataset.dropna(axis=0)\n",
    "# Shuffle the data\n",
    "dataset = dataset.shuffle(random_state=42)\n",
    "\n",
    "# Note that these functions return the Dataset() instance itself (see above), which means you can also chain operations e.g. like this:\n",
    "# dataset.dropna(axis=0).shuffle(seed=42)\n",
    "\n",
    "# Drop the unnecessary columns\n",
    "dataset.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "# Drop highly correlated features\n",
    "dataset.correlation(thr=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis\n",
    "\n",
    "The ```Dataset()``` class offers a ```summary()``` function that can be helpful for EDA. (NOTE: is_categorical is still under development)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aixchem import Dataset\n",
    "\n",
    "\n",
    "# Loading from file.\n",
    "AIxChem = Path.cwd().parents[1]\n",
    "data = AIxChem / \"datasets\" / \"buttar_norrby_dataset.csv\"  # Adjust the path to where the results are located\n",
    "dataset = Dataset(data, target=\"exp_activation_energy\")\n",
    "dataset.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also check column types easily (Enjoy with care):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aixchem import Dataset\n",
    "\n",
    "# Loading from file.\n",
    "AIxChem = Path.cwd().parents[1]\n",
    "data = AIxChem / \"datasets\" / \"buttar_norrby_dataset.csv\"  # Adjust the path to where the results are located\n",
    "dataset = Dataset(data, target=\"exp_activation_energy\")\n",
    "column = dataset.X.columns[0]\n",
    "print(dataset.is_categorical(column))\n",
    "print(dataset.is_numeric(column))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving\n",
    "\n",
    "You can directly save the ```Dataset()``` as an excel file (1 worksheet for data, 1 for labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aixchem import Dataset\n",
    "\n",
    "# Loading from file.\n",
    "AIxChem = Path.cwd().parents[1]\n",
    "data = AIxChem / \"datasets\" / \"buttar_norrby_dataset.csv\"  # Adjust the path to where the results are located\n",
    "dataset = Dataset(data, target=\"exp_activation_energy\")\n",
    "\n",
    "save_path = AIxChem / \"docs/examples\" / \"test.xlsx\"\n",
    "dataset.save(save_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Magic methods\n",
    "\n",
    "The ```Dataset()``` class offers certain magic methods. E.g. Dataset(1) + Dataset(2) will allow you to concatenate 2 datasets, wherease Dataset(1) - Dataset(2) will remove the entries of Dataset(2) from Dataset(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aixchem import Dataset\n",
    "\n",
    "# Loading from file.\n",
    "AIxChem = Path.cwd().parents[1]\n",
    "data = AIxChem / \"datasets\" / \"buttar_norrby_dataset.csv\"  # Adjust the path to where the results are located\n",
    "dataset = Dataset(data, target=\"exp_activation_energy\")\n",
    "\n",
    "print(dataset.X.shape, dataset.y.shape)\n",
    "\n",
    "# Generate subsets\n",
    "d1, d2 = dataset.split(size=.27)\n",
    "print(d1.X.shape, d1.y.shape)\n",
    "print(d2.X.shape, d2.y.shape)\n",
    "\n",
    "# Adding two datasets\n",
    "d = d1 + d2\n",
    "print(d.X.shape, d.y.shape)\n",
    "\n",
    "# Subtracting two datasets\n",
    "d = dataset - d2\n",
    "print(d.X.shape, d.y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other utils\n",
    "\n",
    "Several other functions exist that can be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aixchem import Dataset\n",
    "\n",
    "# Loading from file.\n",
    "AIxChem = Path.cwd().parents[1]\n",
    "data = AIxChem / \"datasets\" / \"buttar_norrby_dataset.csv\"  # Adjust the path to where the results are located\n",
    "dataset = Dataset(data, target=\"exp_activation_energy\")\n",
    "\n",
    "# Splitting the dataset into training and test sets\n",
    "train, test = dataset.split(size=.2)\n",
    "\n",
    "# Copying the dataset\n",
    "dataset_copy = dataset.copy()\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Transformer class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Transformer class allows you to apply classic transformations to your dataset (such as scaling and one-hot encoding) as well as data augmentation. Currently, we have four augmenters in the pipeline\n",
    "\n",
    "1. Additive Gaussian Noise (AGN)\n",
    "2. Nearest Neighbour SMOTE (Sythentic Minority Oversampling Technique)\n",
    "3. Tabular Variational AutoEncoder (TVAE)\n",
    "4. Conditional Tabular GAN (Generative Adversarial Networks)\n",
    "\n",
    "\n",
    "Here an example with AGN..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aixchem import Dataset\n",
    "from aixchem.transform import augment \n",
    "from aixchem.transform import preprocess\n",
    "\n",
    "# Loading from file.\n",
    "AIxChem = Path.cwd().parents[1]\n",
    "data = AIxChem / \"datasets\" / \"buttar_norrby_dataset.csv\"  # Adjust the path to where the results are located\n",
    "train, test = Dataset(data, target=\"exp_activation_energy\").drop(columns=[\"Unnamed: 0\"]).shuffle(random_state=42).dropna(axis=0).split(size=.7, random_state=42)\n",
    "\n",
    "scaler = preprocess.Scaler().fit(train, columns=None)\n",
    "train, test = scaler.transform(train), scaler.transform(test)\n",
    "\n",
    "# if n = 10, 10 augmented instances will be created for each instance in the dataset. Total entries will be n + 10n = 11n \n",
    "aug = augment.AdditiveGaussianNoise(sigma=0.5, n=10, random_state=42).fit(train)\n",
    "\n",
    "# Transform creates augmented instance and add it to the dataset, \n",
    "# returning the augmented dataset (augmented + original)\n",
    "augmented = aug.transform(train)\n",
    "\n",
    "print(f\"Before augmentation: {train.X.shape}, after augmentation: {augmented.X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and here with CTGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aixchem import Dataset\n",
    "from aixchem.transform import augment \n",
    "from aixchem.transform import preprocess\n",
    "\n",
    "# Loading from file.\n",
    "AIxChem = Path.cwd().parents[1]\n",
    "data = AIxChem / \"datasets\" / \"buttar_norrby_dataset.csv\"  # Adjust the path to where the results are located\n",
    "train, test = Dataset(data, target=\"exp_activation_energy\").drop(columns=[\"Unnamed: 0\"]).shuffle(random_state=42).dropna(axis=0).split(size=.7, random_state=42)\n",
    "\n",
    "scaler = preprocess.Scaler().fit(train, columns=None)\n",
    "train, test = scaler.transform(train), scaler.transform(test)\n",
    "\n",
    "aug = augment.CTGAN(n=2, epochs=100, random_state=42, verbose=0).fit(train)\n",
    "\n",
    "# Transform creates augmented instance and add it to the dataset, \n",
    "# returning the augmented dataset (augmented + original)\n",
    "augmented = aug.transform(train)\n",
    "\n",
    "print(f\"Before augmentation: {train.X.shape}, after augmentation: {augmented.X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decomposition Models\n",
    "\n",
    "There are several decomposition algorithms available that allow you to embedd your data into a lateral space. After using the run() method, you will find a new Dataset() instance in the decomposition.embedding attribute. The embedding.X data corresponds to your embedded data, while the embedding.raw attribtue corresponds to the unembedded data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PCA\n",
    "\n",
    "For PCA additional class attributes exist that allow you to access a summary, the loadings and the most important features for each PC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the path to where aixchem is located\n",
    "from aixchem.models import decomposition\n",
    "\n",
    "# Load some dummy dataset\n",
    "from aixchem.test.data import regression_dataset\n",
    "data = regression_dataset()\n",
    "\n",
    "pca = decomposition.PCA(n_components=4).run(data)\n",
    "\n",
    "# Your embedded/unembedded data is stored here:\n",
    "pca.embedding.X, pca.embedding.raw\n",
    "\n",
    "# Other useful information:\n",
    "pca.summary, pca.loadings, pca.feature_ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### UMAP and t-SNE\n",
    "\n",
    "Both of these dont offer any additional information yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aixchem.models import decomposition\n",
    "\n",
    "# Load some dummy dataset\n",
    "from aixchem.test.data import regression_dataset\n",
    "data = regression_dataset()\n",
    "\n",
    "umap = decomposition.UMAP(n_components=2).run(data)\n",
    "tsne = decomposition.tSNE(n_components=2).run(data)\n",
    "\n",
    "umap.embedding.X, tsne.embedding.X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Validator class\n",
    "\n",
    "Validators allow you to easily generate splits of your dataset object. You can use instances of sklearn validators for this. (e.g. kfold for regression, stratifiedKfold for classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from aixchem import Dataset\n",
    "from aixchem.validation.core import CrossValidator\n",
    "\n",
    "\n",
    "# Loading from file.\n",
    "AIxChem = Path.cwd().parents[1]\n",
    "data = AIxChem / \"datasets\" / \"buttar_norrby_dataset.csv\"  # Adjust the path to where the results are located\n",
    "dataset = Dataset(data, target=\"exp_activation_energy\")\n",
    "\n",
    "validator = CrossValidator(splitter=KFold(n_splits=10, shuffle=True, random_state=42))\n",
    "\n",
    "for train, test in validator.split(dataset):\n",
    "    print(train.X.shape, test.X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using a for loop to iterate (sequentially) over each fold from split, you can define a function and run it for each fold in the validator in parallel with the specified number of (logical) cpu cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from aixchem import Dataset\n",
    "from aixchem.validation.core import CrossValidator\n",
    "\n",
    "# Loading from file.\n",
    "AIxChem = Path.cwd().parents[1]\n",
    "data = AIxChem / \"datasets\" / \"buttar_norrby_dataset.csv\"  # Adjust the path to where the results are located\n",
    "dataset = Dataset(data, target=\"exp_activation_energy\")\n",
    "\n",
    "validator = CrossValidator(splitter=KFold(n_splits=4, shuffle=True, random_state=42))\n",
    "\n",
    "\n",
    "def main(train, test):\n",
    "    return train.X.shape, test.X.shape\n",
    "\n",
    "\n",
    "results = validator.run(dataset, main, n_cpus=4)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow example\n",
    "\n",
    "In the following you can see a typical regression workflow. We first load, clean and split our dataset into train and test data.\n",
    "Then we do some transformations (scaling numeric features, encoding categorical features) based on our train data.\n",
    "Finally, we build our model and evaluate it on both, train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aixchem import Dataset\n",
    "from aixchem.transform import preprocess\n",
    "from aixchem.models import regression\n",
    "\n",
    "# Loading from file.\n",
    "AIxChem = Path.cwd().parents[1]\n",
    "data = AIxChem / \"datasets\" / \"buttar_norrby_dataset.csv\"  # Adjust the path to where the results are located\n",
    "dataset = Dataset(data, target=\"exp_activation_energy\").dropna(axis=0).shuffle(random_state=42).drop(columns=[\"Unnamed: 0\"])\n",
    "train, test = dataset.split(size=.75, random_state=42)\n",
    "\n",
    "# Preprocessing with StandardScaler and OneHotEncoder\n",
    "scaler = preprocess.Scaler().fit(train, columns=None)\n",
    "train, test = scaler.transform(train), scaler.transform(test)\n",
    "\n",
    "ohe = preprocess.OneHotEncoder().fit(train, columns=None)\n",
    "train, test = scaler.transform(train), scaler.transform(test)\n",
    "\n",
    "model = regression.RandomForest(n_estimators=1000, max_depth=50, random_state=42).fit(train)\n",
    "\n",
    "print(f\"train: {model.evaluate(train)}, test: {model.evaluate(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example workflow with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aixchem import Dataset\n",
    "from aixchem.transform import preprocess\n",
    "from aixchem.models import regression\n",
    "from sklearn.model_selection import KFold\n",
    "from aixchem.validation.core import CrossValidator\n",
    "\n",
    "# Loading from file.\n",
    "AIxChem = Path.cwd().parents[1]\n",
    "data = AIxChem / \"datasets\" / \"buttar_norrby_dataset.csv\"  # Adjust the path to where the results are located\n",
    "dataset = Dataset(data, target=\"exp_activation_energy\", categorical_thr=None).dropna(axis=0)\n",
    "dataset.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "print(dataset.X.shape, dataset.y.shape)\n",
    "\n",
    "validator = CrossValidator(splitter=KFold(n_splits=4, shuffle=True, random_state=42))\n",
    "\n",
    "for train, test in validator.split(dataset):\n",
    "\n",
    "    scaler = preprocess.Scaler().fit(train, columns=None)\n",
    "    train, test = scaler.transform(train), scaler.transform(test)\n",
    "\n",
    "    print(train.X.shape, test.X.shape)\n",
    "\n",
    "    #ohe = preprocess.OneHotEncoder().fit(train, columns=None)\n",
    "    #train, test = ohe.transform(train), ohe.transform(test)\n",
    "\n",
    "    model = regression.RandomForest(n_estimators=10, max_depth=10, random_state=42).fit(train)\n",
    "\n",
    "    print(f\"train: {model.evaluate(train)}, test: {model.evaluate(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example workflow with cross-validation and parallel execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aixchem import Dataset\n",
    "from aixchem.transform import preprocess\n",
    "from aixchem.models import regression\n",
    "from sklearn.model_selection import KFold\n",
    "from aixchem.validation.core import CrossValidator\n",
    "\n",
    "# Loading from file.\n",
    "AIxChem = Path.cwd().parents[1]\n",
    "data = AIxChem / \"datasets\" / \"buttar_norrby_dataset.csv\"  # Adjust the path to where the results are located\n",
    "dataset = Dataset(data, target=\"exp_activation_energy\", categorical_thr=None).dropna(axis=0)\n",
    "dataset.drop(columns=[\"Unnamed: 0\"])\n",
    "validator = CrossValidator(splitter=KFold(n_splits=4, shuffle=True, random_state=42))\n",
    "\n",
    "\n",
    "def main(train, test):\n",
    "    # Fit and transform the data using Scaler\n",
    "    scaler = preprocess.Scaler().fit(train, columns=None)\n",
    "    train, test = scaler.transform(train), scaler.transform(test)\n",
    "\n",
    "    # Fit and transform the data using OneHotEncoder\n",
    "    #ohe = preprocess.OneHotEncoder().fit(train, columns=None)\n",
    "    #train, test = ohe.transform(train), ohe.transform(test)\n",
    "\n",
    "    # Fit the model\n",
    "    model = regression.RandomForest(n_estimators=1000, max_depth=50, random_state=42).fit(train)\n",
    "\n",
    "    # Evaluate the model\n",
    "    return f\"train: {model.evaluate(train)}, test: {model.evaluate(test)}\"\n",
    "\n",
    "\n",
    "results = validator.run(dataset, main, n_cpus=5)\n",
    "\n",
    "results\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example workflow with the pipeline class, on regression problems\n",
    "\n",
    "A multitude of ML models are available in the model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from aixchem import Dataset\n",
    "from aixchem.validation import CrossValidator\n",
    "from aixchem.pipeline.pipe import Pipeline\n",
    "from aixchem.optimization import GridOptimizer\n",
    "\n",
    "from aixchem.transform import preprocess\n",
    "from aixchem.models import regression\n",
    "\n",
    "# Loading from file.\n",
    "AIxChem = Path.cwd().parents[1]\n",
    "data = AIxChem / \"datasets\" / \"buttar_norrby_dataset.csv\"  # Adjust the path to where the results are located\n",
    "dataset = Dataset(data, target=\"exp_activation_energy\", categorical_thr=None).dropna(axis=0)\n",
    "dataset.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "save_path = AIxChem / \"docs/examples/regression\"\n",
    "\n",
    "pipeline = Pipeline(\n",
    "\n",
    "    dataset=dataset,\n",
    "\n",
    "    transformers=[\n",
    "        preprocess.Scaler(),\n",
    "        #[GridOptimizer(preprocess.OneHotEncoder, params={\"random_state\": [42]}), None]\n",
    "        ],\n",
    "\n",
    "    models={\n",
    "        \"MLR\": regression.LinearModel(),\n",
    "        \"RF\": GridOptimizer(regression.RandomForest, params={\"n_estimators\": [20, 50, 100], \"max_depth\": [10, 20], \"random_state\": [42]}),\n",
    "        \"MLP\": GridOptimizer(regression.NeuralNetwork, params={\n",
    "                        'hidden_neurons': [[46, 46], [92, 92], [92, 46]],\n",
    "                        'optimizer': ['adam', 'rmsprop'],\n",
    "                        'batch_size': [32, 64],\n",
    "                        'epochs': [500, 1000],\n",
    "                        'random_state': [42]})\n",
    "        },\n",
    "\n",
    "    validator=CrossValidator(splitter=KFold(n_splits=3, shuffle=True, random_state=42)),\n",
    "    \n",
    "    # Adjust the path to where you want to save the results\n",
    "    path=save_path\n",
    ")\n",
    "\n",
    "pipeline.run(n_cpus=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example workflow using the Pipeline class for classification. In this example, we perform binary classification by splitting the label into reactions with Experimental Activation Energies above or below the 25 kcal/mol threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from aixchem import Dataset\n",
    "from aixchem.validation import CrossValidator\n",
    "from aixchem.pipeline.pipe import Pipeline\n",
    "from aixchem.optimization import GridOptimizer\n",
    "\n",
    "from aixchem.transform import preprocess\n",
    "from aixchem.models import classification\n",
    "\n",
    "# Loading from file.\n",
    "AIxChem = Path.cwd().parents[1]\n",
    "DATA = AIxChem / \"datasets\" / \"buttar_norrby_dataset.csv\"  # Adjust the path to where the results are located\n",
    "LABEL = \"exp_activation_energy\"\n",
    "BINARY_THR = 25\n",
    "\n",
    "save_path = AIxChem / \"docs/examples/classification\"\n",
    "\n",
    "dataset = Dataset(DATA, target=LABEL, categorical_thr=None).dropna(axis=0).shuffle(random_state=42)\n",
    "# encode the label as binary\n",
    "dataset.y[LABEL] = dataset.y[LABEL].apply(lambda x: 0 if x <= BINARY_THR else 1)\n",
    "\n",
    "dataset.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "pipeline = Pipeline(\n",
    "\n",
    "    dataset=dataset,\n",
    "\n",
    "    transformers=[\n",
    "        preprocess.Scaler(),\n",
    "        #[GridOptimizer(preprocess.OneHotEncoder, params={\"random_state\": [42]}), None]\n",
    "        ],\n",
    "\n",
    "    models={\n",
    "        \"LOG\": classification.LogModel(),\n",
    "        \"RF\": GridOptimizer(classification.RandomForest, params={\"n_estimators\": [20, 50, 100], \"max_depth\": [10, 20], \"random_state\": [42]}),\n",
    "        },\n",
    "\n",
    "    validator=CrossValidator(splitter=KFold(n_splits=3, shuffle=True, random_state=42)),\n",
    "    \n",
    "    # Adjust the path to where you want to save the results\n",
    "    path=save_path\n",
    ")\n",
    "\n",
    "pipeline.run(n_cpus=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of simultaneous optimization of augmenters and ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from aixchem import Dataset\n",
    "from aixchem.validation import CrossValidator\n",
    "from aixchem.pipeline.pipe import Pipeline\n",
    "from aixchem.optimization import GridOptimizer\n",
    "\n",
    "from aixchem.transform import preprocess\n",
    "from aixchem.models import regression\n",
    "\n",
    "\n",
    "# Loading from file.\n",
    "AIxChem = Path.cwd().parents[1]\n",
    "DATA = AIxChem / \"datasets\" / \"buttar_norrby_dataset.csv\"  # Adjust the path to where the results are located\n",
    "dataset = Dataset(DATA, target=\"exp_activation_energy\", categorical_thr=None).dropna(axis=0)\n",
    "dataset.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "save_path = AIxChem / \"docs/examples/regression_augmentation\"\n",
    "\n",
    "pipeline = Pipeline(\n",
    "\n",
    "    dataset=dataset,\n",
    "\n",
    "    transformers=[\n",
    "        preprocess.Scaler(),\n",
    "\n",
    "        # Augmentation\n",
    "        [ \n",
    "            None, \n",
    "            GridOptimizer(\n",
    "                augment.AdditiveGaussianNoise, params={\"n\":[1, 2, 3], \"sigma\":[0.1, 0.5, 1.0], \"random_state\":[42], \"transform_y\":[True]},\n",
    "            ),\n",
    "        ],\n",
    "    ],\n",
    "\n",
    "    models={\n",
    "        \"MLR\": regression.LinearModel(),\n",
    "        \"RF\": GridOptimizer(regression.RandomForest, params={\"n_estimators\": [20, 50, 100], \"max_depth\": [10, 20], \"random_state\": [42]}),\n",
    "        },\n",
    "\n",
    "    validator=CrossValidator(splitter=KFold(n_splits=3, shuffle=True, random_state=42)),\n",
    "    \n",
    "    # Adjust the path to where you want to save the results\n",
    "    path=save_path\n",
    ")\n",
    "\n",
    "pipeline.run(n_cpus=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aixchem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

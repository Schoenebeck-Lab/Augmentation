{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIxChem\n",
    "### Visual exploration\n",
    "#### Datasets — Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.colors\n",
    "\n",
    "from aixchem import Dataset\n",
    "from aixchem.transform import preprocess\n",
    "from aixchem.models import decomposition\n",
    "from aixchem.plots.scatter import Scatter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "AIxChem = Path.cwd().parent\n",
    "DATA = AIxChem / \"datasets\" / \"buttar_norrby_dataset.csv\"\n",
    "LABELS = \"exp_activation_energy\"\n",
    "\n",
    "# Load the data & clean it\n",
    "data = Dataset(DATA, target=LABELS, sep=\",\").dropna(axis=0).shuffle(random_state=42)\n",
    "# Drop the index column\n",
    "data.X.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "# Drop highly correlated features\n",
    "data.correlation(thr=0.8)\n",
    "\n",
    "scaled = preprocess.Scaler().fit(data).transform(data)\n",
    "\n",
    "embeddings = {\n",
    "    \"pca\": decomposition.PCA(n_components=2).run(scaled).embedding,\n",
    "    \"umap\": decomposition.UMAP(n_components=2).run(scaled).embedding,\n",
    "    \"tsne\": decomposition.tSNE(n_components=2).run(scaled).embedding,\n",
    "}\n",
    "\n",
    "\n",
    "#fig, ax = plt.subplots(3, 1, figsize=(4, 4*3))\n",
    "fig, ax = plt.subplots(1, 3, figsize=(4*3, 4))\n",
    "#cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [\"#800040\", \"#DFE8F3\",\"#004080\"])\n",
    "cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [\"#C8D4E3\", \"#004080\"])\n",
    "fig.tight_layout()\n",
    "\n",
    "for idx, (embeddor, e) in enumerate(embeddings.items()):\n",
    "\n",
    "    scatter = Scatter(\n",
    "        x=e.X.iloc[:, 0], \n",
    "        y=e.X.iloc[:, 1], \n",
    "        colors=e.y[f\"{LABELS}\"],\n",
    "        sizes=8,\n",
    "        ax=ax[idx],\n",
    "        colormap=cmap\n",
    "        )\n",
    "\n",
    "    scatter.set_axis_style(xlabel=e.X.iloc[:, 0].name, ylabel=e.X.iloc[:, 1].name)\n",
    "\n",
    "# After creating all plots\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"embeddings.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.colors\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from aixchem import Dataset\n",
    "from aixchem.transform import preprocess\n",
    "from aixchem.models import decomposition\n",
    "from aixchem.plots.scatter import Scatter\n",
    "\n",
    "\n",
    "AIxChem = Path.cwd().parent\n",
    "DATA = AIxChem / \"datasets\" / \"buttar_norrby_dataset.csv\"\n",
    "LABELS = \"exp_activation_energy\"\n",
    "\n",
    "# Load the data & clean it\n",
    "data = Dataset(DATA, target=LABELS, sep=\",\").dropna(axis=0).shuffle(random_state=42)\n",
    "# Drop the index column\n",
    "data.X.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "# Drop highly correlated features\n",
    "data.correlation(thr=0.8)\n",
    "\n",
    "\n",
    "embeddings = {\n",
    "    \"pca\": decomposition.PCA(n_components=2).run(scaled).embedding,\n",
    "    \"umap\": decomposition.UMAP(n_components=2).run(scaled).embedding,\n",
    "    \"tsne\": decomposition.tSNE(n_components=2).run(scaled).embedding,\n",
    "}\n",
    "\n",
    "\n",
    "#fig, ax = plt.subplots(3, 1, figsize=(4, 4*3))\n",
    "fig, ax = plt.subplots(1, 3, figsize=(4*3, 4))\n",
    "#cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [\"#800040\", \"#DFE8F3\",\"#004080\"])\n",
    "cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [\"#C8D4E3\", \"#004080\"])\n",
    "fig.tight_layout()\n",
    "\n",
    "for idx, (embeddor, e) in enumerate(embeddings.items()):\n",
    "\n",
    "    scatter = Scatter(\n",
    "        x=e.X.iloc[:, 0], \n",
    "        y=e.X.iloc[:, 1], \n",
    "        colors=e.y[f\"{LABELS}\"],\n",
    "        sizes=8,\n",
    "        ax=ax[idx],\n",
    "        colormap=cmap\n",
    "        )\n",
    "\n",
    "    scatter.set_axis_style(xlabel=e.X.iloc[:, 0].name, ylabel=e.X.iloc[:, 1].name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perfomance Plot\n",
    "\n",
    "Compare augmented and non-augmented performance using a performance plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from aixchem.plots.core import Plot\n",
    "\n",
    "# Define the path to the results\n",
    "AIxChem = Path.cwd().parent\n",
    "RESULTS = AIxChem / \"docs/buttar_norrby_results/holdout_RMSE\"\n",
    "\n",
    "# Define the metric to plot\n",
    "METRIC = \"holdout_RMSE\"\n",
    "\n",
    "color_map = {\n",
    "    \"AGN\": \"#1f77b4\",\n",
    "    \"NNSMOTE\": \"#ff7f0e\",\n",
    "    \"TVAE\": \"#2ca02c\",\n",
    "    \"CTGAN\": \"#d62728\",\n",
    "}\n",
    "\n",
    "# Read all csv files from the dir\n",
    "files = [f for f in RESULTS.iterdir() if f.suffix == \".csv\" and not \"noaug\" in f.stem and not \"std\" in f.stem and not \"FOLD\" in f.stem]\n",
    "\n",
    "# Group results according to model\n",
    "results = {model: [f for f in files if model in f.stem] for model in set([f.stem.split(\"_\")[1] for f in files])}\n",
    "\n",
    "# Handle both single and multiple subplot cases\n",
    "n_models = len(results.keys())\n",
    "fig, axes = plt.subplots(1, n_models, figsize=(4.5*n_models, 4), squeeze=False)\n",
    "axes = axes.flatten()  # Flatten in case of single subplot\n",
    "\n",
    "\n",
    "for idx, (model, result) in enumerate(results.items()):\n",
    "    # Create model plot\n",
    "    plot = Plot(ax=axes[idx])\n",
    "\n",
    "    unaugmented_df = pd.read_csv(f\"{result[0].parent / '_'.join(result[0].stem.split('_')[:2])}_noaug.csv\", index_col=0)\n",
    "    #unaugmented_df.drop(labels=\"5%\", inplace=True)  # Drop the 5% row\n",
    "    plot.ax.plot(unaugmented_df[\"FRAC\"], unaugmented_df[METRIC], label=\"Baseline\", color=\"#000000\", linestyle=\"--\")\n",
    "    for r in result:\n",
    "        df = pd.read_csv(r, index_col=0)\n",
    "        # df.drop(labels=\"5%\", inplace=True) # Drop the 5% row\n",
    "        label = r.stem.split(\"_\")[0]\n",
    "        color = color_map.get(label, \"#000000\")  # Default to black if label not in color_map\n",
    "\n",
    "        plot.ax.plot(df[\"FRAC\"], df[METRIC], label=r.stem.split(\"_\")[0], marker=\"s\", markersize=3, color=color, alpha=0.5)\n",
    "        plot.ax.set_title(model)\n",
    "        # Show legend\n",
    "        plot.ax.legend()\n",
    "        # Set axis limits\n",
    "        plot.ax.set_ylim(1, 3.5)        \n",
    "        plot.ax.set_xticks(df[\"FRAC\"])\n",
    "        plot.ax.set_xticklabels(df.index)\n",
    "        plot.ax.set_xlabel(\"Fraction of train data (75%)\")\n",
    "        plot.ax.set_ylabel(METRIC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bar plot with standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from aixchem.plots.core import Plot\n",
    "\n",
    "\n",
    "# Define the path to the results and the metric to plot\n",
    "AIxChem = Path.cwd().parent\n",
    "RESULTS = AIxChem / \"docs/buttar_norrby_results/holdout_RMSE\"\n",
    "METRIC = \"holdout_RMSE\"\n",
    "SORT = \"MAX\" \n",
    " \n",
    "# Read all csv files from the dir\n",
    "files = [f for f in RESULTS.iterdir() if f.suffix == \".csv\" and not \"noaug\" in f.stem and not \"std\" in f.stem]\n",
    "\n",
    "# Group results according to model\n",
    "results = {model: [f for f in files if model in f.stem] for model in set([f.stem.split(\"_\")[1] for f in files])}\n",
    "\n",
    "# Handle both single and multiple subplot cases\n",
    "n_models = len(results.keys())\n",
    "fig, axes = plt.subplots(1, n_models, figsize=(4.5*n_models, 4), squeeze=False)\n",
    "axes = axes.flatten()  # Flatten in case of single subplot\n",
    "\n",
    "for idx, (model, result) in enumerate(results.items()):\n",
    "\n",
    "    # Load non-augmented mean data\n",
    "    unaugmented_df = pd.read_csv(f\"{result[0].parent / '_'.join(result[0].stem.split('_')[:2])}_noaug.csv\", index_col=0)\n",
    "    \n",
    "    # Load non-augmented standard deviation data\n",
    "    try:\n",
    "        unaugmented_std_df = pd.read_csv(f\"{result[0].parent / '_'.join(result[0].stem.split('_')[:2])}_noaug_std.csv\", index_col=0)\n",
    "    except FileNotFoundError:\n",
    "        # Create a DataFrame with zeros if std file not found\n",
    "        unaugmented_std_df = pd.DataFrame(0, index=unaugmented_df.index, columns=unaugmented_df.columns)\n",
    "\n",
    "    # Load augmented mean data\n",
    "    augmented_dfs = []\n",
    "    augmented_std_dfs = []\n",
    "    \n",
    "    # Pair each mean file with its std file\n",
    "    for r in result:\n",
    "        mean_df = pd.read_csv(r, index_col=0)\n",
    "        augmented_dfs.append(mean_df)\n",
    "        \n",
    "        # Try to load std file\n",
    "        try:\n",
    "            std_df = pd.read_csv(r.parent / f\"{r.stem}_std.csv\", index_col=0)\n",
    "            augmented_std_dfs.append(std_df)\n",
    "        except FileNotFoundError:\n",
    "            # Create a DataFrame with zeros if std file not found\n",
    "            augmented_std_dfs.append(pd.DataFrame(0, index=mean_df.index, columns=mean_df.columns))\n",
    "\n",
    "    # Create a combined DataFrame for all augmenters for each percentage\n",
    "    aug_all = pd.concat(augmented_dfs)\n",
    "    \n",
    "    # Find best augmenter for each data fraction\n",
    "    best_augs = {}\n",
    "    best_augs_std = {}\n",
    "    \n",
    "    for data_frac in unaugmented_df.index:\n",
    "        # Filter all augmenter results for this data fraction\n",
    "        aug_frac_data = []\n",
    "        for i, df in enumerate(augmented_dfs):\n",
    "            if data_frac in df.index:\n",
    "                aug_frac_data.append((i, df.loc[data_frac, METRIC]))\n",
    "        \n",
    "        # Find best augmenter (min or max)\n",
    "        if len(aug_frac_data) > 0:\n",
    "            if SORT == \"MIN\":\n",
    "                best_idx, best_val = min(aug_frac_data, key=lambda x: x[1])\n",
    "            else:\n",
    "                best_idx, best_val = max(aug_frac_data, key=lambda x: x[1])\n",
    "            \n",
    "            # Store best value and corresponding std\n",
    "            best_augs[data_frac] = best_val\n",
    "            if data_frac in augmented_std_dfs[best_idx].index:\n",
    "                best_augs_std[data_frac] = augmented_std_dfs[best_idx].loc[data_frac, METRIC]\n",
    "            else:\n",
    "                best_augs_std[data_frac] = 0  # Default if std not available\n",
    "        else:\n",
    "            best_augs[data_frac] = np.nan\n",
    "            best_augs_std[data_frac] = 0\n",
    "\n",
    "    # Create DataFrames for plotting\n",
    "    augmented_best = pd.DataFrame({METRIC: best_augs}, index=unaugmented_df.index)\n",
    "    std_best = pd.DataFrame({METRIC: best_augs_std}, index=unaugmented_df.index)\n",
    "\n",
    "    plot = Plot(ax=axes[idx])\n",
    "\n",
    "    x = np.arange(len(unaugmented_df.index))  # the label locations\n",
    "    y_aug = augmented_best[METRIC]\n",
    "    y_noaug = unaugmented_df[METRIC]\n",
    "    \n",
    "    # Get standard deviations\n",
    "    y_aug_std = std_best[METRIC]\n",
    "    y_noaug_std = unaugmented_std_df[METRIC]\n",
    "\n",
    "    # Plot with error bars\n",
    "    plot.ax.bar(x - 0.2, y_noaug, 0.4, label=\"Baseline\", yerr=y_noaug_std, capsize=5)\n",
    "    plot.ax.bar(x + 0.2, y_aug, 0.4, label=\"Best Aug.\", yerr=y_aug_std, capsize=5)\n",
    "    plot.ax.legend(loc='upper right')\n",
    "\n",
    "    plot.ax.set_title(model)\n",
    "    plot.ax.set_xticks(x)\n",
    "    plot.ax.set_xticklabels(unaugmented_df.index)\n",
    "    plot.ax.set_ylim(1, 4) \n",
    "    plot.ax.set_ylabel(METRIC)\n",
    "    plot.ax.set_xlabel(\"Fraction of train data (75%)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hat Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from aixchem.plots.core import Plot\n",
    "\n",
    "\n",
    "def performance_plot(results_path, metric, sort_max=True, drop_index=[], lower_is_better=None):\n",
    "    # Automatically determine if lower is better based on metric name if not specified\n",
    "    if lower_is_better is None:\n",
    "        # Common metrics where lower values are better\n",
    "        lower_better_metrics = [\"RMSE\", \"MAE\", \"MSE\", \"loss\", \"error\"]\n",
    "        lower_is_better = any(m in metric for m in lower_better_metrics)\n",
    "    \n",
    "    # Read all csv files from the dir\n",
    "    files = [f for f in results_path.iterdir() if f.suffix == \".csv\" and not \"noaug\" in f.stem and not \"std\" in f.stem and not \"FOLD\" in f.stem]\n",
    "\n",
    "    # Group results according to model\n",
    "    results = {model: [f for f in files if model in f.stem] for model in set([f.stem.split(\"_\")[1] for f in files])}\n",
    "\n",
    "    # Handle both single and multiple subplot cases\n",
    "    n_models = len(results.keys())\n",
    "    fig, axes = plt.subplots(1, n_models, figsize=(4.5*n_models, 4), squeeze=False)\n",
    "    axes = axes.flatten()  # Flatten in case of single subplot\n",
    "\n",
    "    for idx, (model, result) in enumerate(results.items()):\n",
    "\n",
    "        # Get unaugmented results\n",
    "        unaugmented_df = pd.read_csv(f\"{result[0].parent / '_'.join(result[0].stem.split('_')[:2])}_noaug.csv\", index_col=0)\n",
    "\n",
    "        # Get augmented results\n",
    "        augmented_dfs = [pd.read_csv(r, index_col=0) for r in result]\n",
    "\n",
    "        # Aggregate to get the best augmented result - invert sort_max if lower is better\n",
    "        effective_sort = sort_max if not lower_is_better else not sort_max\n",
    "        \n",
    "        if effective_sort:\n",
    "            augmented_best = pd.concat(augmented_dfs, keys=range(len(augmented_dfs))).groupby(level=1).max()\n",
    "        else:\n",
    "            augmented_best = pd.concat(augmented_dfs, keys=range(len(augmented_dfs))).groupby(level=1).min()\n",
    "\n",
    "        # Ensure both have the same order\n",
    "        augmented_best = augmented_best.reindex(unaugmented_df.index)\n",
    "\n",
    "        if drop_index:\n",
    "            unaugmented_df = unaugmented_df.drop(index=drop_index)\n",
    "            augmented_best = augmented_best.drop(index=drop_index)\n",
    "\n",
    "        # Create model plot\n",
    "        plot = Plot(ax=axes[idx])\n",
    "\n",
    "        xs = np.arange(len(unaugmented_df.index))\n",
    "        ys_aug = augmented_best[metric]\n",
    "        ys_noaug = unaugmented_df[metric]\n",
    "\n",
    "        # Find the optimal baseline for comparison\n",
    "        if lower_is_better:\n",
    "            optimal_baseline = ys_noaug.min()\n",
    "        else:\n",
    "            optimal_baseline = ys_noaug.max()\n",
    "\n",
    "        for x, y_noaug, y_aug in zip(xs, ys_noaug, ys_aug):\n",
    "            \n",
    "            # Plot horizontal line to represent base performance\n",
    "            hline_offset = 0.25\n",
    "            plot.ax.hlines(y_noaug, x - hline_offset*2, x + hline_offset, color=\"#A2B1C6\", linestyle=\"-\")\n",
    "            \n",
    "            # For RMSE: Lower is better, so improvement is when y_aug < y_noaug\n",
    "            # For R2: Higher is better, so improvement is when y_aug > y_noaug\n",
    "            is_improvement = (y_aug < y_noaug) if lower_is_better else (y_aug > y_noaug)\n",
    "            \n",
    "            # Plot performance difference as bar starting from hline\n",
    "            bar_color = \"#004080\" if is_improvement else \"#800040\"\n",
    "            plot.ax.bar(x, y_aug - y_noaug, 2*hline_offset, bottom=y_noaug, color=bar_color)\n",
    "\n",
    "            # Calculate percentage improvement - different formula for different metrics\n",
    "            if lower_is_better:\n",
    "                # For RMSE: (noaug - aug)/noaug = percentage reduction\n",
    "                pct_change = ((y_noaug - y_aug) / y_noaug) * 100 if y_noaug != 0 else 0\n",
    "            else:\n",
    "                # For R2: (aug - noaug)/noaug = percentage improvement\n",
    "                pct_change = ((y_aug - y_noaug) / abs(y_noaug)) * 100 if y_noaug != 0 else 0\n",
    "            \n",
    "            # Annotate with percentage improvement\n",
    "            is_exceeding_optimal = (y_aug < optimal_baseline) if lower_is_better else (y_aug > optimal_baseline)\n",
    "            font_weight = \"bold\" if is_exceeding_optimal else \"normal\"\n",
    "            \n",
    "            if is_improvement:\n",
    "                plot.ax.annotate(f\"{pct_change:.1f}%\", (x, y_aug), textcoords=\"offset points\", \n",
    "                               xytext=(0, -1 if lower_is_better else 7), \n",
    "                               ha='center', va='top' if lower_is_better else 'baseline', \n",
    "                               fontweight=font_weight, fontsize=10, color=bar_color, rotation=90)\n",
    "            else:\n",
    "                plot.ax.annotate(f\"{pct_change:.1f}%\", (x, y_noaug), textcoords=\"offset points\", \n",
    "                               xytext=(0, 7 if lower_is_better else -1), \n",
    "                               ha='center', va='baseline' if lower_is_better else 'top', \n",
    "                               fontweight=font_weight, fontsize=10, color=bar_color, rotation=90)\n",
    "\n",
    "        # Add line indicating the optimal performance of the unaugmented model\n",
    "        plot.ax.hlines(optimal_baseline, xs[0] - 0.5, xs[-1] + 0.5, color=\"#A2B1C6\", linestyle=\":\")\n",
    "        \n",
    "        # Add label for the optimal value\n",
    "        optimal_label = \"Min\" if lower_is_better else \"Max\"\n",
    "        plot.ax.annotate(f\"{optimal_label}: {optimal_baseline:.2f}\", (xs[0], optimal_baseline), \n",
    "                       textcoords=\"offset points\", xytext=(0, 2), \n",
    "                       ha='left', va='bottom', fontsize=10, color=\"#A2B1C6\")\n",
    "\n",
    "        # layout\n",
    "        plot.ax.set_title(model)\n",
    "        plot.ax.set_xticks(xs)\n",
    "        plot.ax.set_xticklabels(ys_noaug.index)\n",
    "        \n",
    "        # Set appropriate y-limits based on data\n",
    "        y_min = min(min(ys_aug), min(ys_noaug)) * 0.7  # Add some padding\n",
    "        y_max = max(max(ys_aug), max(ys_noaug)) * 1.2\n",
    "        plot.ax.set_ylim(y_min, y_max)\n",
    "        \n",
    "        plot.ax.set_ylabel(metric)\n",
    "        plot.ax.set_xlabel(\"Fraction of train data (75%)\")\n",
    "\n",
    "# Define the path to the results\n",
    "AIxChem = Path.cwd().parent\n",
    "RESULTS = AIxChem / \"docs/buttar_norrby_results/holdout_RMSE\"\n",
    "\n",
    "performance_plot(\n",
    "    results_path=RESULTS, \n",
    "    metric=\"holdout_RMSE\", \n",
    "    sort_max=True,\n",
    "    lower_is_better=True  # Explicitly set that for RMSE lower is better\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the statistical difference between the augmented and non-augmented results. If the p-values for a certain fraction are below 0.05 this means that the performances coming from the augmented dataset are statistically better or worse than the non-augmented performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "\n",
    "def get_best_augmented_performance(results_path, metric, sort_max=True, drop_index=[]):\n",
    "    \"\"\"\n",
    "    This function goes through the folder where the mean results are stored and gets the best augmenter per fraction of dataset considered for all the different models\n",
    "\n",
    "    :param results_path: path to the folder where the mean results (CSV files) are stored\n",
    "    :param metric: metric used to sort the performances of the models\n",
    "\n",
    "    Returns\n",
    "    - best_performances: it is a dictionary that has as top-keys the ML models. Second level keys are the percentage of the data used, to each of these keys corresponds \n",
    "                         two values: name of the best augmenter, mean_value\n",
    "    \"\"\"\n",
    "    # Read all csv files from the dir\n",
    "    files = [f for f in results_path.iterdir() if f.suffix == \".csv\" and not \"noaug\" in f.stem and not \"std\" in f.stem and not \"FOLD\" in f.stem]\n",
    "\n",
    "    # Group results according to model\n",
    "    results = {model: [f for f in files if model in f.stem] for model in set([f.stem.split(\"_\")[1] for f in files])}\n",
    "\n",
    "    best_performances = {}\n",
    "\n",
    "    for model, result in results.items():\n",
    "        # Get unaugmented results\n",
    "        unaugmented_df = pd.read_csv(f\"{result[0].parent / '_'.join(result[0].stem.split('_')[:2])}_noaug.csv\", index_col=0)\n",
    "\n",
    "        # Get augmented results\n",
    "        augmented_dfs = [pd.read_csv(r, index_col=0) for r in result]\n",
    "\n",
    "        # Aggregate to get the best augmented result\n",
    "        if sort_max:\n",
    "            augmented_best = pd.concat(augmented_dfs, keys=range(len(augmented_dfs)), names=['file_index', 'row_index']).groupby(level=1).max()\n",
    "        else:\n",
    "            augmented_best = pd.concat(augmented_dfs, keys=range(len(augmented_dfs)), names=['file_index', 'row_index']).groupby(level=1).min()\n",
    "\n",
    "        # Ensure both have the same order\n",
    "        augmented_best = augmented_best.reindex(unaugmented_df.index)\n",
    "\n",
    "        # drop the specified indexes\n",
    "        if drop_index:\n",
    "            print(\"Indexes to drop:\", drop_index)\n",
    "            unaugmented_df = unaugmented_df.drop(index=drop_index)\n",
    "            augmented_best = augmented_best.drop(index=drop_index)\n",
    "\n",
    "        best_performances[model] = {}\n",
    "        # iterate through the percentage of augmented best\n",
    "        for idx in augmented_best.index:\n",
    "            # Get the best value for the current percentage\n",
    "            best_value = augmented_best.loc[idx, metric]\n",
    "            best_file_index = None\n",
    "            best_file_name = None\n",
    "\n",
    "            # Iterate through the augmented DataFrames to find the file that contains the best value\n",
    "            for file_index, df in enumerate(augmented_dfs):\n",
    "                \n",
    "                # Check if the current percentage is in the DataFrame and if the value matches the best value\n",
    "                if idx in df.index and df.loc[idx, metric] == best_value:\n",
    "                    best_file_index = file_index\n",
    "                    # Keep only the first part of the file name\n",
    "                    best_file_name = result[file_index].stem.split(\"_aug\")[0] \n",
    "                    break\n",
    "             # If the best file index was found, store the best value and file name in the dictionary\n",
    "            if best_file_index is not None:\n",
    "                best_performances[model][idx] = (best_value, best_file_name)\n",
    "            else:\n",
    "                print(f\"Warning: Best file for index {idx} not found in model {model}.\")\n",
    "\n",
    "    return best_performances\n",
    "\n",
    "\n",
    "def get_performance_per_fold(folds_path, metric, best_performances, n_folds: int):\n",
    "    \"\"\"\n",
    "    This function, according to the information stored in \"best_performances\", creates a dictionary where the results per model/percentage/best mean sequences are stored.\n",
    "    \"\"\"\n",
    "    noaug_per_fold = {}\n",
    "    aug_per_fold = {}\n",
    "\n",
    "    # iterate through the top level keys (models).\n",
    "    for model in best_performances.keys():\n",
    "        noaug_per_fold[model] = {}\n",
    "        aug_per_fold[model] = {}\n",
    "        \n",
    "        # Get the folder list\n",
    "        fold_list = [f\"FOLD{i}\" for i in range(n_folds)]\n",
    "        \n",
    "        # Get file lists once outside the loops\n",
    "        noaug_files = [f for f in folds_path.iterdir() if f.suffix == \".csv\" and \"noaug\" in f.stem and \"FOLD\" in f.stem]\n",
    "        \n",
    "        # Read the non-augmented data file once\n",
    "        if noaug_files:\n",
    "            noaug_df = pd.read_csv(noaug_files[0], index_col=0)\n",
    "            \n",
    "        # Process each percentage\n",
    "        for percentage in best_performances[model]:\n",
    "            if percentage not in noaug_per_fold[model]:\n",
    "                noaug_per_fold[model][percentage] = {}\n",
    "            if percentage not in aug_per_fold[model]:\n",
    "                aug_per_fold[model][percentage] = {}\n",
    "                \n",
    "            # Get the augmenter dataset info\n",
    "            performance, dataset = best_performances[model][percentage]\n",
    "            \n",
    "            # Get the augmented data file for this dataset\n",
    "            aug_files = [f for f in folds_path.iterdir() if f.suffix == \".csv\" and dataset in f.stem and not \"noaug\" in f.stem and \"FOLD\" in f.stem]\n",
    "            \n",
    "            # Read the augmented data file once\n",
    "            if aug_files:\n",
    "                aug_df = pd.read_csv(aug_files[0], index_col=0)\n",
    "            \n",
    "            # For each fold, extract the fold-specific data\n",
    "            for fold in fold_list:\n",
    "\n",
    "                if noaug_files:\n",
    "\n",
    "                    mask_noaug = (noaug_df['FOLD'] == fold) & (noaug_df.index.str.contains(percentage))\n",
    "                    filtered_noaug = noaug_df[mask_noaug]\n",
    "                    \n",
    "                    if not filtered_noaug.empty:\n",
    "                        performance_noaug = filtered_noaug[metric].iloc[0]  # Take first match\n",
    "                        noaug_per_fold[model][percentage][fold] = performance_noaug\n",
    "                \n",
    "\n",
    "                if aug_files:\n",
    "                    mask_aug = (aug_df['FOLD'] == fold) & (aug_df.index.str.contains(percentage))\n",
    "                    filtered_aug = aug_df[mask_aug]\n",
    "                    \n",
    "                    if not filtered_aug.empty:\n",
    "                        performance_aug = filtered_aug[metric].iloc[0]  # Take first match\n",
    "                        aug_per_fold[model][percentage][fold] = performance_aug\n",
    "\n",
    "\n",
    "    return noaug_per_fold, aug_per_fold\n",
    "                            \n",
    "\n",
    "def calculate_p_values(noaug_per_fold, aug_per_fold, lower_is_better=True):\n",
    "    \"\"\"\n",
    "    Calculate one-tailed p-values to compare the performance of augmented and non-augmented data.\n",
    "    \n",
    "    For metrics where lower is better (e.g., RMSE, MSE):\n",
    "    - We want to test if augmented is better than non-augmented (aug < noaug)\n",
    "    \n",
    "    For metrics where higher is better (e.g., R², accuracy):\n",
    "    - We want to test if augmented is better than non-augmented (aug > noaug)\n",
    "    \n",
    "    :param noaug_per_fold: Dictionary containing results for non-augmented data\n",
    "    :param aug_per_fold: Dictionary containing results for augmented data\n",
    "    :param lower_is_better: Whether lower values indicate better performance (default True for RMSE-like metrics)\n",
    "    \n",
    "    Returns:\n",
    "    - p_values: Dictionary with models as keys and another dictionary as values, \n",
    "                where the inner dictionary has percentages as keys and p-values as values.\n",
    "    - t_stats: Dictionary with the t-statistics for each comparison.\n",
    "    \"\"\"\n",
    "    \n",
    "    p_values = {}\n",
    "    t_stats = {}  # To store t-statistics\n",
    "    \n",
    "    # Iterate over each model in the noaug_per_fold dictionary\n",
    "    for model in noaug_per_fold.keys():\n",
    "        p_values[model] = {}\n",
    "        t_stats[model] = {}\n",
    "        \n",
    "        # Iterate over each percentage in the noaug_per_fold dictionary for the current model\n",
    "        for percentage in noaug_per_fold[model].keys():\n",
    "            # Extract the noaug values for the current model and percentage across all folds\n",
    "            noaug_values = [noaug_per_fold[model][percentage][fold] for fold in noaug_per_fold[model][percentage].keys()]\n",
    "            # Extract the augmented values for the current model and percentage across all folds\n",
    "            aug_values = [aug_per_fold[model][percentage][fold] for fold in aug_per_fold[model][percentage].keys()]\n",
    "\n",
    "            # Ensure both noaug_values and aug_values have more than one element to perform the test\n",
    "            if len(noaug_values) > 1 and len(aug_values) > 1:\n",
    "                # Calculate the t-statistic and two-tailed p-value using a paired t-test\n",
    "                # For RMSE (lower_is_better=True): noaug - aug > 0 means aug is better (lower)\n",
    "                # For R² (lower_is_better=False): noaug - aug < 0 means aug is better (higher)\n",
    "                t_stat, two_tailed_p_value = ttest_rel(noaug_values, aug_values)\n",
    "                \n",
    "                # Determine if we're testing if augmentation is better\n",
    "                # For lower_is_better=True: we want to test if aug < noaug (positive t-stat)\n",
    "                # For lower_is_better=False: we want to test if aug > noaug (negative t-stat)\n",
    "                if lower_is_better:\n",
    "                    # One-tailed p-value for H₁: μ₁ > μ₂ (noaug > aug)\n",
    "                    one_tailed_p_value = two_tailed_p_value / 2 if t_stat > 0 else 1 - (two_tailed_p_value / 2)\n",
    "                else:\n",
    "                    # One-tailed p-value for H₁: μ₁ < μ₂ (noaug < aug)\n",
    "                    one_tailed_p_value = two_tailed_p_value / 2 if t_stat < 0 else 1 - (two_tailed_p_value / 2)\n",
    "                \n",
    "                # Store the one-tailed p-value and t-stat in the respective dictionaries\n",
    "                p_values[model][percentage] = one_tailed_p_value\n",
    "                t_stats[model][percentage] = t_stat\n",
    "            else:\n",
    "                # Handle case where there are not enough data points to run the test\n",
    "                p_values[model][percentage] = None\n",
    "                t_stats[model][percentage] = None\n",
    "    \n",
    "    return p_values, t_stats\n",
    "\n",
    "\n",
    "def plot_p_values(p_values):\n",
    "    \"\"\"\n",
    "    Plot p-values for each model in a single figure with multiple subplots.\n",
    "\n",
    "    - p_values: Dictionary with models as keys and another dictionary as values, \n",
    "                where the inner dictionary has percentages as keys and p-values as values.\n",
    "    \"\"\"\n",
    "    # Sort the models alphabetically\n",
    "    sorted_models = sorted(p_values.keys())\n",
    "    \n",
    "    # Determine the number of models\n",
    "    n_models = len(sorted_models)  # Changed from num_models to n_models for consistency\n",
    "    \n",
    "    # Create figure with appropriate number of subplots\n",
    "    fig, axes = plt.subplots(1, n_models, figsize=(4.5*n_models, 4), squeeze=False)\n",
    "    axes = axes.flatten()  # Flatten in case of single subplot\n",
    "    \n",
    "    # Plot each model's p-values in a separate subplot\n",
    "    for idx, model in enumerate(sorted_models):\n",
    "        percentages = p_values[model]\n",
    "        \n",
    "        # Filter out None values\n",
    "        valid_percentages = {k: v for k, v in percentages.items() if v is not None}\n",
    "        \n",
    "        ax = axes[idx]\n",
    "        ax.bar(valid_percentages.keys(), valid_percentages.values())\n",
    "        ax.axhline(y=0.05, color='r', linestyle='--', label='Significance Threshold (0.05)')\n",
    "        ax.set_xlabel('Percentage')\n",
    "        ax.set_ylabel('P-value')\n",
    "        ax.set_title(model)\n",
    "        ax.legend()\n",
    "    \n",
    "    plt.ylim(0, 1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def p_values_to_csv(p_values, output_path):\n",
    "    \"\"\"\n",
    "    Save p-values to a CSV file.\n",
    "\n",
    "    - p_values: Dictionary with models as keys and another dictionary as values, \n",
    "                where the inner dictionary has percentages as keys and p-values as values.\n",
    "    - output_path: Path to save the CSV file.\n",
    "    \"\"\"\n",
    "    # Create a DataFrame from the p_values dictionary\n",
    "    p_values_df = pd.DataFrame.from_dict({(model, percentage): p_value for model, percentages in p_values.items() for percentage, p_value in percentages.items()}, orient='index', columns=['P-value'])\n",
    "    \n",
    "    # Reset index to have model and percentage as columns\n",
    "    p_values_df.reset_index(inplace=True)\n",
    "    \n",
    "    # Rename columns\n",
    "    p_values_df.columns = ['Model_Percentage', 'P-value']\n",
    "    \n",
    "    # Split the 'Model_Percentage' column into 'Model' and 'Percentage'\n",
    "    p_values_df[['Model', 'Percentage']] = pd.DataFrame(p_values_df['Model_Percentage'].tolist(), index=p_values_df.index)\n",
    "    \n",
    "    # Drop the 'Model_Percentage' column\n",
    "    p_values_df.drop(columns=['Model_Percentage'], inplace=True)\n",
    "    \n",
    "    # Reorder columns\n",
    "    p_values_df = p_values_df[['Model', 'Percentage', 'P-value']]\n",
    "    \n",
    "    # Save to CSV\n",
    "    p_values_df.to_csv(output_path, index=False)\n",
    "\n",
    "\n",
    "\n",
    "# Define the path to the results\n",
    "AIxChem = Path.cwd().parent\n",
    "results = AIxChem / \"docs/buttar_norrby_results/holdout_RMSE\"\n",
    "# scoring metric\n",
    "metric = \"holdout_RMSE\"\n",
    "# number of folds\n",
    "n_folds = 20\n",
    "\n",
    "best_performances = get_best_augmented_performance(results_path=results, metric=metric)\n",
    "\n",
    "noaug_per_fold, aug_per_fold = get_performance_per_fold(folds_path=results, metric=metric, best_performances=best_performances, n_folds=n_folds)\n",
    "\n",
    "p_values, t_stat = calculate_p_values(noaug_per_fold=noaug_per_fold, aug_per_fold=aug_per_fold, lower_is_better=True)\n",
    "                                                 \n",
    "plot_p_values(p_values=p_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 -steps TOST\n",
    "## Step 1. \n",
    "We are comparing the not-augmented results with themself, in order to understand when is the Plateau reached. \n",
    "In this example, it is possible to observe that RMSE values coming from 90% oh training size is statistically indistinguishable from the RMSE values coming from 100% of the training size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.weightstats import ttost_ind\n",
    "\n",
    "\n",
    "def get_best_augmented_performance(results_path, metric, sort_max=True, drop_index=[]):\n",
    "    \"\"\"\n",
    "    This function goes through the folder where the mean results are stored and gets the best augmenter per fraction of dataset considered for all the different models\n",
    "\n",
    "    :param results_path: path to the folder where the mean results (CSV files) are stored\n",
    "    :param metric: metric used to sort the performances of the models\n",
    "\n",
    "    Returns\n",
    "    - best_performances: it is a dictionary that has as top-keys the ML models. Second level keys are the percentage of the data used, to each of these keys corresponds \n",
    "                         two values: name of the best augmenter, mean_value\n",
    "    \"\"\"\n",
    "    # Read all csv files from the dir\n",
    "    files = [f for f in results_path.iterdir() if f.suffix == \".csv\" and not \"noaug\" in f.stem and not \"std\" in f.stem and not \"FOLD\" in f.stem]\n",
    "\n",
    "    # Group results according to model\n",
    "    results = {model: [f for f in files if model in f.stem] for model in set([f.stem.split(\"_\")[1] for f in files])}\n",
    "\n",
    "    best_performances = {}\n",
    "\n",
    "    for model, result in results.items():\n",
    "        # Get unaugmented results\n",
    "        unaugmented_df = pd.read_csv(f\"{result[0].parent / '_'.join(result[0].stem.split('_')[:2])}_noaug.csv\", index_col=0)\n",
    "\n",
    "        # Get augmented results\n",
    "        augmented_dfs = [pd.read_csv(r, index_col=0) for r in result]\n",
    "\n",
    "        # Aggregate to get the best augmented result\n",
    "        if sort_max:\n",
    "            augmented_best = pd.concat(augmented_dfs, keys=range(len(augmented_dfs)), names=['file_index', 'row_index']).groupby(level=1).max()\n",
    "        else:\n",
    "            augmented_best = pd.concat(augmented_dfs, keys=range(len(augmented_dfs)), names=['file_index', 'row_index']).groupby(level=1).min()\n",
    "\n",
    "        # Ensure both have the same order\n",
    "        augmented_best = augmented_best.reindex(unaugmented_df.index)\n",
    "\n",
    "        # drop the specified indexes\n",
    "        if drop_index:\n",
    "            print(\"Indexes to drop:\", drop_index)\n",
    "            unaugmented_df = unaugmented_df.drop(index=drop_index)\n",
    "            augmented_best = augmented_best.drop(index=drop_index)\n",
    "\n",
    "        best_performances[model] = {}\n",
    "        # iterate through the percentage of augmented best\n",
    "        for idx in augmented_best.index:\n",
    "            # Get the best value for the current percentage\n",
    "            best_value = augmented_best.loc[idx, metric]\n",
    "            best_file_index = None\n",
    "            best_file_name = None\n",
    "\n",
    "            # Iterate through the augmented DataFrames to find the file that contains the best value\n",
    "            for file_index, df in enumerate(augmented_dfs):\n",
    "                \n",
    "                # Check if the current percentage is in the DataFrame and if the value matches the best value\n",
    "                if idx in df.index and df.loc[idx, metric] == best_value:\n",
    "                    best_file_index = file_index\n",
    "                    # Keep only the first part of the file name\n",
    "                    best_file_name = result[file_index].stem.split(\"_aug\")[0] \n",
    "                    break\n",
    "             # If the best file index was found, store the best value and file name in the dictionary\n",
    "            if best_file_index is not None:\n",
    "                best_performances[model][idx] = (best_value, best_file_name)\n",
    "            else:\n",
    "                print(f\"Warning: Best file for index {idx} not found in model {model}.\")\n",
    "\n",
    "    return best_performances\n",
    "\n",
    "def best_performance_per_model(best_unaugmented_performances, metric):\n",
    "    \"\"\"\n",
    "    Identifies the best performance value for each model across all data fractions.\n",
    "    \n",
    "    Parameters:\n",
    "    - best_unaugmented_performances: Dictionary with models as keys and another dictionary \n",
    "      as values, where inner dictionaries have percentages/fractions as keys and \n",
    "      performance values as values.\n",
    "    - metric: Performance metric to optimize (e.g., \"holdout_R2\", \"holdout_RMSE\", \"holdout_MAE\")\n",
    "    \n",
    "    Returns:\n",
    "    - best_performance_unaug: Dictionary with models as keys and their best performance \n",
    "      value as values, optimized according to the metric direction.\n",
    "    \"\"\"\n",
    "    best_performance_unaug = {}\n",
    "    \n",
    "    if metric == \"holdout_R2\":\n",
    "        # For R², higher values are better\n",
    "        for model in best_unaugmented_performances.keys():\n",
    "            best_performance_unaug[model] = max(best_unaugmented_performances[model].values())\n",
    "\n",
    "    elif metric == \"holdout_RMSE\" or metric == \"holdout_MAE\":\n",
    "        # For RMSE and MAE, lower values are better\n",
    "        for model in best_unaugmented_performances.keys():\n",
    "            best_performance_unaug[model] = min(best_unaugmented_performances[model].values())\n",
    "\n",
    "    return best_performance_unaug\n",
    "\n",
    "def get_performance_per_fold(folds_path, metric, best_performances, n_folds: int):\n",
    "    \"\"\"\n",
    "    This function, according to the information stored in \"best_performances\", creates a dictionary where the results per model/percentage/best mean sequences are stored.\n",
    "    \"\"\"\n",
    "    noaug_per_fold = {}\n",
    "    aug_per_fold = {}\n",
    "\n",
    "    # iterate through the top level keys (models).\n",
    "    for model in best_performances.keys():\n",
    "        noaug_per_fold[model] = {}\n",
    "        aug_per_fold[model] = {}\n",
    "        \n",
    "        # Get the folder list\n",
    "        fold_list = [f\"FOLD{i}\" for i in range(n_folds)]\n",
    "        \n",
    "        # Get file lists once outside the loops\n",
    "        noaug_files = [f for f in folds_path.iterdir() if f.suffix == \".csv\" and \"noaug\" in f.stem and \"FOLD\" in f.stem]\n",
    "        \n",
    "        # Read the non-augmented data file once\n",
    "        if noaug_files:\n",
    "            noaug_df = pd.read_csv(noaug_files[0], index_col=0)\n",
    "            \n",
    "        # Process each percentage\n",
    "        for percentage in best_performances[model]:\n",
    "            if percentage not in noaug_per_fold[model]:\n",
    "                noaug_per_fold[model][percentage] = {}\n",
    "            if percentage not in aug_per_fold[model]:\n",
    "                aug_per_fold[model][percentage] = {}\n",
    "                \n",
    "            # Get the augmenter dataset info\n",
    "            performance, dataset = best_performances[model][percentage]\n",
    "            \n",
    "            # Get the augmented data file for this dataset\n",
    "            aug_files = [f for f in folds_path.iterdir() if f.suffix == \".csv\" and dataset in f.stem and not \"noaug\" in f.stem and \"FOLD\" in f.stem]\n",
    "            \n",
    "            # Read the augmented data file once\n",
    "            if aug_files:\n",
    "                aug_df = pd.read_csv(aug_files[0], index_col=0)\n",
    "            \n",
    "            # For each fold, extract the fold-specific data\n",
    "            for fold in fold_list:\n",
    "\n",
    "                if noaug_files:\n",
    "\n",
    "                    mask_noaug = (noaug_df['FOLD'] == fold) & (noaug_df.index.str.contains(percentage))\n",
    "                    filtered_noaug = noaug_df[mask_noaug]\n",
    "                    \n",
    "                    if not filtered_noaug.empty:\n",
    "                        performance_noaug = filtered_noaug[metric].iloc[0]  # Take first match\n",
    "                        noaug_per_fold[model][percentage][fold] = performance_noaug\n",
    "                \n",
    "\n",
    "                if aug_files:\n",
    "                    mask_aug = (aug_df['FOLD'] == fold) & (aug_df.index.str.contains(percentage))\n",
    "                    filtered_aug = aug_df[mask_aug]\n",
    "                    \n",
    "                    if not filtered_aug.empty:\n",
    "                        performance_aug = filtered_aug[metric].iloc[0]  # Take first match\n",
    "                        aug_per_fold[model][percentage][fold] = performance_aug\n",
    "\n",
    "\n",
    "    return noaug_per_fold, aug_per_fold\n",
    "\n",
    "def get_noaug_per_fold100(folds_path, metric, best_performances, n_folds: int):\n",
    "    \"\"\"\n",
    "    This function creates a dictionary with unaugmented performance at 100% data fraction\n",
    "    for each model and percentage combination across all folds.\n",
    "    \n",
    "    :param folds_path: path to the folder where the results CSV is stored\n",
    "    :param metric: metric used to evaluate model performance\n",
    "    :param best_performances: dictionary coming from get_best_augmented_performance\n",
    "    :param n_folds: number of folds in the dataset\n",
    "    \n",
    "    Returns:\n",
    "    - noaug_per_fold100: dictionary with models as top-keys, percentages as second-level keys,\n",
    "                         and fold-specific performance values at 100% data fraction\n",
    "    \"\"\"\n",
    "    noaug_per_fold100 = {}\n",
    "    \n",
    "    # Find the single noaug file with FOLD data\n",
    "    noaug_files = [f for f in folds_path.iterdir() if f.suffix == \".csv\" and \"noaug\" in f.stem and \"FOLD\" in f.stem]\n",
    "    \n",
    "    if not noaug_files:\n",
    "        print(\"No noaug FOLD files found.\")\n",
    "        return noaug_per_fold100\n",
    "    \n",
    "    # Read the single CSV containing all fold data\n",
    "    noaug_df = pd.read_csv(noaug_files[0], index_col=0)\n",
    "    \n",
    "    # Get list of fold identifiers\n",
    "    fold_list = [f\"FOLD{i}\" for i in range(n_folds)]\n",
    "    \n",
    "    # Iterate through the models\n",
    "    for model in best_performances.keys():\n",
    "        noaug_per_fold100[model] = {}\n",
    "        \n",
    "        # Iterate through the percentages for the current model\n",
    "        for percentage in best_performances[model]:\n",
    "            noaug_per_fold100[model][percentage] = {}\n",
    "            \n",
    "            # Filter the dataframe for 100% data fraction rows that match the model\n",
    "            mask_100 = (noaug_df.index.str.contains(\"100%\")) & (noaug_df.index.str.contains(model))\n",
    "            filtered_100 = noaug_df[mask_100]\n",
    "            \n",
    "            if filtered_100.empty:\n",
    "                print(f\"No 100% data fraction found for model {model}\")\n",
    "                continue\n",
    "            \n",
    "            # For each fold, extract the performance value at 100% data fraction\n",
    "            for fold in fold_list:\n",
    "                fold_mask = filtered_100['FOLD'] == fold\n",
    "                fold_data = filtered_100[fold_mask]\n",
    "                \n",
    "                if not fold_data.empty:\n",
    "                    performance_noaug = fold_data[metric].iloc[0]  # Take first match\n",
    "                    noaug_per_fold100[model][percentage][fold] = performance_noaug\n",
    "    \n",
    "    return noaug_per_fold100\n",
    "\n",
    "def get_std_unaugmented_performances(mean_path, metric):\n",
    "    \"\"\"\n",
    "    Get the standard deviation of the unaugmented performances for each model\n",
    "    \n",
    "    :param mean_path: path to the folder where the results are stored (CSV files)\n",
    "    :param metric: metric used to evaluate model performance\n",
    "    \"\"\"\n",
    "    std_noaug = {}\n",
    "    \n",
    "    # Get only files with pattern AGN_{model}_noaug_std.csv\n",
    "    noaug_files = [f for f in mean_path.iterdir() \n",
    "                   if f.suffix == \".csv\" and \n",
    "                   f.stem.startswith(\"AGN_\") and \n",
    "                   \"noaug_std\" in f.stem]\n",
    "    \n",
    "    # Process each file to extract the model name\n",
    "    for file_path in noaug_files:\n",
    "        # Extract model name - it's the second part after splitting by \"_\"\n",
    "        parts = file_path.stem.split(\"_\")\n",
    "        if len(parts) >= 2:\n",
    "            model = parts[1]  # Get model name (RF, SVM, etc.)\n",
    "            \n",
    "            # Read the CSV file\n",
    "            noaug_df = pd.read_csv(file_path, index_col=0)\n",
    "            \n",
    "            if \"100%\" in noaug_df.index:\n",
    "                # Get the standard deviation for the unaugmented model at 100% fraction\n",
    "                performance_noaug = noaug_df.loc[\"100%\", metric]\n",
    "                \n",
    "                # Store the performance in the dictionary\n",
    "                std_noaug[model] = performance_noaug\n",
    "    \n",
    "    return std_noaug\n",
    "                            \n",
    "def perform_tost(noaug_per_fold100, noaug_per_fold, metric, standard_deviation):\n",
    "    \"\"\"\n",
    "    Perform the Two One-Sided T-Test (TOST), also known as the equivalence test.\n",
    "    Skip comparisons when percentages are 100% (to avoid comparing identical distributions).\n",
    "    \n",
    "    Null hypothesis: m1 - m2 < low or m1 - m2 > upp \n",
    "    Alternative hypothesis: low < m1 - m2 < upp\n",
    "    \n",
    "    :param noaug_per_fold100: dictionary with the unaugmented performances at 100%\n",
    "    :param noaug_per_fold: dictionary with the unaugmented performances at various percentages\n",
    "    :param metric: the metric to be used for the TOST\n",
    "    :param standard_deviation: dictionary with standard deviation for each model\n",
    "    \"\"\"\n",
    "    p_values = {}\n",
    "\n",
    "    # iterate through the models\n",
    "    for model in noaug_per_fold100.keys():\n",
    "        p_values[model] = {}\n",
    "\n",
    "        # Calculate the lower and upper bounds for the TOST\n",
    "        lower_bound = -(standard_deviation[model])\n",
    "        upper_bound = standard_deviation[model]\n",
    "\n",
    "        # iterate through the percentages\n",
    "        for percentage in noaug_per_fold100[model].keys():\n",
    "            # Skip the 100% comparison with itself\n",
    "            if percentage == \"100%\":\n",
    "                p_values[model][percentage] = None  # or np.nan or any other marker\n",
    "                continue\n",
    "                \n",
    "            # Extract the noaug values for the current model and percentage across all folds\n",
    "            noaug_values = [noaug_per_fold100[model][percentage][fold] for fold in noaug_per_fold100[model][percentage].keys()]\n",
    "            # Extract the augmented values for the current model and percentage across all folds\n",
    "            aug_values = [noaug_per_fold[model][percentage][fold] for fold in noaug_per_fold[model][percentage].keys()]\n",
    "\n",
    "            # perform the TOST\n",
    "            p_value, pv1, pv2 = ttost_ind(noaug_values, aug_values, low=lower_bound, upp=upper_bound, usevar='unequal')\n",
    "\n",
    "            # store the p-value\n",
    "            p_values[model][percentage] = p_value\n",
    "\n",
    "    return p_values, pv1, pv2\n",
    "\n",
    "def plot_p_values(p_values):\n",
    "    \"\"\"\n",
    "    Plot p-values for each model in a single figure with multiple subplots.\n",
    "    Handles both single and multiple models.\n",
    "\n",
    "    - p_values: Dictionary with models as keys and another dictionary as values, \n",
    "                where the inner dictionary has percentages as keys and p-values as values.\n",
    "    \"\"\"\n",
    "    # Sort the models alphabetically\n",
    "    sorted_models = sorted(p_values.keys())\n",
    "    \n",
    "    # Determine the number of models\n",
    "    num_models = len(sorted_models)\n",
    "    \n",
    "    # Create a figure with subplots in one row\n",
    "    fig, axes = plt.subplots(1, num_models, figsize=(5 * num_models, 5), sharey=True)\n",
    "    \n",
    "    # Handle the case of a single model (axes is not an array in this case)\n",
    "    if num_models == 1:\n",
    "        axes = [axes]  # Convert to a list with a single element for consistent indexing\n",
    "    \n",
    "    # Plot each model's p-values in a separate subplot\n",
    "    for idx, model in enumerate(sorted_models):\n",
    "        percentages = p_values[model]\n",
    "        \n",
    "        # Filter out None values\n",
    "        valid_percentages = {k: v for k, v in percentages.items() if v is not None}\n",
    "        \n",
    "        ax = axes[idx]\n",
    "        ax.bar(valid_percentages.keys(), valid_percentages.values())\n",
    "        ax.axhline(y=0.05, color='r', linestyle='--', label='Significance Threshold (0.05)')\n",
    "        ax.set_xlabel('Percentage')\n",
    "        ax.set_ylabel('P-value')\n",
    "        ax.set_title(model)\n",
    "        ax.legend()\n",
    "    \n",
    "    # Adjust layout\n",
    "    style = Path.cwd().parent / \"aixchem/plots/style.mpl\"\n",
    "    plt.style.use(style)\n",
    "    plt.rcParams['font.monospace'] = 'DejaVu Sans Mono'\n",
    "    plt.ylim(0, 1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def p_values_to_csv(p_values, output_path):\n",
    "    \"\"\"\n",
    "    Save p-values to a CSV file.\n",
    "\n",
    "    - p_values: Dictionary with models as keys and another dictionary as values, \n",
    "                where the inner dictionary has percentages as keys and p-values as values.\n",
    "    - output_path: Path to save the CSV file.\n",
    "    \"\"\"\n",
    "    # Create a DataFrame from the p_values dictionary\n",
    "    p_values_df = pd.DataFrame.from_dict({(model, percentage): p_value for model, percentages in p_values.items() for percentage, p_value in percentages.items()}, orient='index', columns=['P-value'])\n",
    "    \n",
    "    # Reset index to have model and percentage as columns\n",
    "    p_values_df.reset_index(inplace=True)\n",
    "    \n",
    "    # Rename columns\n",
    "    p_values_df.columns = ['Model_Percentage', 'P-value']\n",
    "    \n",
    "    # Split the 'Model_Percentage' column into 'Model' and 'Percentage'\n",
    "    p_values_df[['Model', 'Percentage']] = pd.DataFrame(p_values_df['Model_Percentage'].tolist(), index=p_values_df.index)\n",
    "    \n",
    "    # Drop the 'Model_Percentage' column\n",
    "    p_values_df.drop(columns=['Model_Percentage'], inplace=True)\n",
    "    \n",
    "    # Reorder columns\n",
    "    p_values_df = p_values_df[['Model', 'Percentage', 'P-value']]\n",
    "    \n",
    "    # Save to CSV\n",
    "    p_values_df.to_csv(output_path, index=False)\n",
    "\n",
    "# Define the path to the results\n",
    "AIxChem = Path.cwd().parent\n",
    "results = AIxChem / \"docs/buttar_norrby_results/holdout_RMSE\"\n",
    "# # scoring metric\n",
    "metric = \"holdout_RMSE\"\n",
    "# number of folds\n",
    "n_folds = 20\n",
    "\n",
    "best_performances = get_best_augmented_performance(results_path=results, metric=metric)\n",
    "\n",
    "std_noaug = get_std_unaugmented_performances(mean_path=results, metric=metric)\n",
    "\n",
    "noaug_per_fold, aug_per_fold = get_performance_per_fold(folds_path=results, metric=metric, best_performances=best_performances, n_folds=n_folds)\n",
    "\n",
    "noaug_per_fold100 = get_noaug_per_fold100(folds_path=results, metric=metric, best_performances=best_performances, n_folds=n_folds)\n",
    "\n",
    "p_values, pv1, pv2 = perform_tost(noaug_per_fold100=noaug_per_fold100, noaug_per_fold=noaug_per_fold, metric=metric, standard_deviation=std_noaug)\n",
    "\n",
    "print(p_values)\n",
    "                                                    \n",
    "plot_p_values(p_values=p_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-Steps TOST\n",
    "## Second Step\n",
    "\n",
    "In this example we are comparing the RMSE values coming from 90% of the non-augmented training size with the RMSE values coming from the different fractions of the augmented dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.weightstats import ttost_ind\n",
    "\n",
    "\n",
    "def get_best_augmented_performance(results_path, metric, sort_max=True, drop_index=[]):\n",
    "    \"\"\"\n",
    "    This function goes through the folder where the mean results are stored and gets the best augmenter per fraction of dataset considered for all the different models\n",
    "\n",
    "    :param results_path: path to the folder where the mean results (CSV files) are stored\n",
    "    :param metric: metric used to sort the performances of the models\n",
    "\n",
    "    Returns\n",
    "    - best_performances: it is a dictionary that has as top-keys the ML models. Second level keys are the percentage of the data used, to each of these keys corresponds \n",
    "                         two values: name of the best augmenter, mean_value\n",
    "    \"\"\"\n",
    "    # Read all csv files from the dir\n",
    "    files = [f for f in results_path.iterdir() if f.suffix == \".csv\" and not \"noaug\" in f.stem and not \"std\" in f.stem and not \"FOLD\" in f.stem]\n",
    "\n",
    "    # Group results according to model\n",
    "    results = {model: [f for f in files if model in f.stem] for model in set([f.stem.split(\"_\")[1] for f in files])}\n",
    "\n",
    "    best_performances = {}\n",
    "\n",
    "    for model, result in results.items():\n",
    "        # Get unaugmented results\n",
    "        unaugmented_df = pd.read_csv(f\"{result[0].parent / '_'.join(result[0].stem.split('_')[:2])}_noaug.csv\", index_col=0)\n",
    "\n",
    "        # Get augmented results\n",
    "        augmented_dfs = [pd.read_csv(r, index_col=0) for r in result]\n",
    "\n",
    "        # Aggregate to get the best augmented result\n",
    "        if sort_max:\n",
    "            augmented_best = pd.concat(augmented_dfs, keys=range(len(augmented_dfs)), names=['file_index', 'row_index']).groupby(level=1).max()\n",
    "        else:\n",
    "            augmented_best = pd.concat(augmented_dfs, keys=range(len(augmented_dfs)), names=['file_index', 'row_index']).groupby(level=1).min()\n",
    "\n",
    "        # Ensure both have the same order\n",
    "        augmented_best = augmented_best.reindex(unaugmented_df.index)\n",
    "\n",
    "        # drop the specified indexes\n",
    "        if drop_index:\n",
    "            print(\"Indexes to drop:\", drop_index)\n",
    "            unaugmented_df = unaugmented_df.drop(index=drop_index)\n",
    "            augmented_best = augmented_best.drop(index=drop_index)\n",
    "\n",
    "        best_performances[model] = {}\n",
    "        # iterate through the percentage of augmented best\n",
    "        for idx in augmented_best.index:\n",
    "            # Get the best value for the current percentage\n",
    "            best_value = augmented_best.loc[idx, metric]\n",
    "            best_file_index = None\n",
    "            best_file_name = None\n",
    "\n",
    "            # Iterate through the augmented DataFrames to find the file that contains the best value\n",
    "            for file_index, df in enumerate(augmented_dfs):\n",
    "                \n",
    "                # Check if the current percentage is in the DataFrame and if the value matches the best value\n",
    "                if idx in df.index and df.loc[idx, metric] == best_value:\n",
    "                    best_file_index = file_index\n",
    "                    # Keep only the first part of the file name\n",
    "                    best_file_name = result[file_index].stem.split(\"_aug\")[0] \n",
    "                    break\n",
    "             # If the best file index was found, store the best value and file name in the dictionary\n",
    "            if best_file_index is not None:\n",
    "                best_performances[model][idx] = (best_value, best_file_name)\n",
    "            else:\n",
    "                print(f\"Warning: Best file for index {idx} not found in model {model}.\")\n",
    "\n",
    "    return best_performances\n",
    "\n",
    "def get_performance_per_fold(folds_path, metric, best_performances, n_folds: int, best_percentage_per_model: dict):\n",
    "    \"\"\"\n",
    "    This function extracts fold-specific performance values from a single CSV file \n",
    "    where different folds are identified by a 'FOLD' column.\n",
    "    \n",
    "    :param folds_path: path to the folder containing CSV files with 'FOLD' column\n",
    "    :param metric: metric used to evaluate model performance\n",
    "    :param best_performances: dictionary from get_best_augmented_performance\n",
    "    :param n_folds: number of folds in the dataset\n",
    "    :param best_percentage_per_model: dictionary with best percentage fraction for each model\n",
    "    \n",
    "    Returns:\n",
    "    - noaug_per_fold: dictionary with unaugmented performance values per model/percentage/fold\n",
    "    - aug_per_fold: dictionary with augmented performance values per model/percentage/fold\n",
    "    \"\"\"\n",
    "    noaug_per_fold = {}\n",
    "    aug_per_fold = {}\n",
    "    \n",
    "    # Find the single noaug file with FOLD data\n",
    "    noaug_files = [f for f in folds_path.iterdir() if f.suffix == \".csv\" and \"noaug\" in f.stem and \"FOLD\" in f.stem]\n",
    "    \n",
    "    if not noaug_files:\n",
    "        print(\"No noaug FOLD file found.\")\n",
    "        return noaug_per_fold, aug_per_fold\n",
    "        \n",
    "    # Read the noaug CSV file\n",
    "    noaug_df = pd.read_csv(noaug_files[0], index_col=0)\n",
    "    \n",
    "    # Generate fold identifiers\n",
    "    fold_list = [f\"FOLD{i}\" for i in range(n_folds)]\n",
    "    \n",
    "    # iterate through the models\n",
    "    for model in best_performances.keys():\n",
    "        noaug_per_fold[model] = {}\n",
    "        aug_per_fold[model] = {}\n",
    "        \n",
    "        # Get the best percentage for this model\n",
    "        best_frac = best_percentage_per_model.get(model)\n",
    "        \n",
    "        # Process each percentage from best_performances\n",
    "        for percentage in best_performances[model]:\n",
    "            noaug_per_fold[model][percentage] = {}\n",
    "            aug_per_fold[model][percentage] = {}\n",
    "            \n",
    "            # Get augmenter information\n",
    "            _, dataset = best_performances[model][percentage]\n",
    "            \n",
    "            # Find the augmented data file for this dataset\n",
    "            aug_files = [f for f in folds_path.iterdir() \n",
    "                        if f.suffix == \".csv\" and dataset in f.stem and \n",
    "                        not \"noaug\" in f.stem and \"FOLD\" in f.stem]\n",
    "                \n",
    "            if not aug_files:\n",
    "                print(f\"No augmented FOLD file found for dataset {dataset}\")\n",
    "                continue\n",
    "                \n",
    "            # Read the augmented data file\n",
    "            aug_df = pd.read_csv(aug_files[0], index_col=0)\n",
    "            \n",
    "            # Process each fold\n",
    "            for fold in fold_list:\n",
    "                # For unaugmented data: filter by fold and model, use best_frac percentage\n",
    "                noaug_mask = (\n",
    "                    (noaug_df['FOLD'] == fold) & \n",
    "                    (noaug_df.index.str.contains(model)) &\n",
    "                    (noaug_df.index.str.contains(best_frac))\n",
    "                )\n",
    "                \n",
    "                noaug_filtered = noaug_df[noaug_mask]\n",
    "                \n",
    "                if not noaug_filtered.empty:\n",
    "                    performance_noaug = noaug_filtered[metric].iloc[0]\n",
    "                    noaug_per_fold[model][percentage][fold] = performance_noaug\n",
    "                else:\n",
    "                    print(f\"No unaugmented data found for model {model}, fold {fold}, fraction {best_frac}\")\n",
    "                \n",
    "                # For augmented data: filter by fold, model, and percentage\n",
    "                aug_mask = (\n",
    "                    (aug_df['FOLD'] == fold) & \n",
    "                    (aug_df.index.str.contains(model)) &\n",
    "                    (aug_df.index.str.contains(percentage))\n",
    "                )\n",
    "                \n",
    "                aug_filtered = aug_df[aug_mask]\n",
    "                \n",
    "                if not aug_filtered.empty:\n",
    "                    performance_aug = aug_filtered[metric].iloc[0]\n",
    "                    aug_per_fold[model][percentage][fold] = performance_aug\n",
    "                else:\n",
    "                    print(f\"No augmented data found for model {model}, fold {fold}, percentage {percentage}\")\n",
    "    \n",
    "    return noaug_per_fold, aug_per_fold\n",
    "\n",
    "def get_std_unaugmented_performances(mean_path, metric, best_percentage_per_model: dict):\n",
    "    \"\"\"\n",
    "    Get the standard deviation of unaugmented performances for each model\n",
    "    \n",
    "    :param mean_path: path to the folder where the standard deviation CSV files are stored\n",
    "    :param metric: metric used to evaluate model performance\n",
    "    :param best_percentage_per_model: dictionary with best percentage fraction for each model\n",
    "    \"\"\"\n",
    "    std_noaug = {}\n",
    "    \n",
    "    # Get only files with pattern AGN_{model}_noaug_std.csv\n",
    "    noaug_files = [f for f in mean_path.iterdir() \n",
    "                  if f.suffix == \".csv\" and \n",
    "                  f.stem.startswith(\"AGN_\") and \n",
    "                  \"noaug_std\" in f.stem]\n",
    "    \n",
    "    print(\"Found std files:\", noaug_files)\n",
    "    \n",
    "    # Process each file to extract the model name\n",
    "    for file_path in noaug_files:\n",
    "        # Extract model name - it's the second part after splitting by \"_\"\n",
    "        parts = file_path.stem.split(\"_\")\n",
    "        if len(parts) >= 2:\n",
    "            model = parts[1]  # Get model name (RF, SVM, etc.)\n",
    "            \n",
    "            # Read the CSV file\n",
    "            noaug_df = pd.read_csv(file_path, index_col=0)\n",
    "            \n",
    "            # Get the best fraction for this model\n",
    "            fraction = best_percentage_per_model.get(model)\n",
    "            \n",
    "            if fraction in noaug_df.index:\n",
    "                # Get standard deviation for the selected percentage fraction\n",
    "                performance_noaug = noaug_df.loc[fraction, metric]\n",
    "                \n",
    "                # Store the standard deviation in the dictionary\n",
    "                std_noaug[model] = performance_noaug\n",
    "            else:\n",
    "                print(f\"Warning: Model {model} doesn't have fraction {fraction} in std file\")\n",
    "    \n",
    "    return std_noaug\n",
    "                            \n",
    "def perform_tost(noaug_per_fold, aug_per_fold, metric, standard_deviation):\n",
    "    \"\"\"\n",
    "    Perform the Two One-Sided T-Test (TOST), also known as the equivalence test. \n",
    "    \n",
    "    Null hypothesis: m1 - m2 < low or m1 - m2 > upp alternative hypothesis: low < m1 - m2 < upp\n",
    "    \n",
    "    :param noaug_per_fold: dictionary with the unaugmented performances\n",
    "    :param aug_per_fold: dictionary with the augmented performances\n",
    "    :param metric: the metric to be used for the TOST\n",
    "    :param best_performance: dictionary with the best performance for each model\n",
    "    \"\"\"\n",
    "\n",
    "    p_values = {}\n",
    "\n",
    "    # iterate through the models\n",
    "    for model in noaug_per_fold.keys():\n",
    "        p_values[model] = {}\n",
    "\n",
    "        # Calculate the lower and upper bounds for the TOST\n",
    "        lower_bound = -(standard_deviation[model])\n",
    "        upper_bound = standard_deviation[model]\n",
    "\n",
    "        # iterate through the percentages\n",
    "        for percentage in noaug_per_fold[model].keys():\n",
    "            # Extract the noaug values for the current model and percentage across all folds\n",
    "            noaug_values = [noaug_per_fold[model][percentage][fold] for fold in noaug_per_fold[model][percentage].keys()]\n",
    "            # Extract the augmented values for the current model and percentage across all folds\n",
    "            aug_values = [aug_per_fold[model][percentage][fold] for fold in aug_per_fold[model][percentage].keys()]\n",
    "\n",
    "            # perform the TOST\n",
    "            p_value, pv1, pv2 = ttost_ind(noaug_values, aug_values, low=lower_bound, upp=upper_bound, usevar='unequal')\n",
    "\n",
    "            # store the p-value\n",
    "            p_values[model][percentage] = p_value\n",
    "\n",
    "    return p_values, pv1, pv2\n",
    "\n",
    "def plot_p_values(p_values):\n",
    "    \"\"\"\n",
    "    Plot p-values for each model in a single figure with multiple subplots.\n",
    "    Handles both single and multiple models.\n",
    "\n",
    "    - p_values: Dictionary with models as keys and another dictionary as values, \n",
    "                where the inner dictionary has percentages as keys and p-values as values.\n",
    "    \"\"\"\n",
    "    # Sort the models alphabetically\n",
    "    sorted_models = sorted(p_values.keys())\n",
    "    \n",
    "    # Determine the number of models\n",
    "    num_models = len(sorted_models)\n",
    "    \n",
    "    # Create a figure with subplots in one row\n",
    "    fig, axes = plt.subplots(1, num_models, figsize=(5 * num_models, 5), sharey=True)\n",
    "    \n",
    "    # Handle the case of a single model (axes is not an array in this case)\n",
    "    if num_models == 1:\n",
    "        axes = [axes]  # Convert to a list with a single element for consistent indexing\n",
    "    \n",
    "    # Plot each model's p-values in a separate subplot\n",
    "    for idx, model in enumerate(sorted_models):\n",
    "        percentages = p_values[model]\n",
    "        \n",
    "        # Filter out None values\n",
    "        valid_percentages = {k: v for k, v in percentages.items() if v is not None}\n",
    "        \n",
    "        ax = axes[idx]\n",
    "        ax.bar(valid_percentages.keys(), valid_percentages.values())\n",
    "        ax.axhline(y=0.05, color='r', linestyle='--', label='Significance Threshold (0.05)')\n",
    "        ax.set_xlabel('Percentage')\n",
    "        ax.set_ylabel('P-value')\n",
    "        ax.set_title(model)\n",
    "        ax.legend()\n",
    "    \n",
    "    # Adjust layout\n",
    "    # Define the path to the results\n",
    "    style = Path.cwd().parent / \"aixchem/plots/style.mpl\"\n",
    "    plt.style.use(style)\n",
    "    plt.rcParams['font.monospace'] = 'DejaVu Sans Mono'\n",
    "    plt.ylim(0, 1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def p_values_to_csv(p_values, output_path):\n",
    "    \"\"\"\n",
    "    Save p-values to a CSV file.\n",
    "\n",
    "    - p_values: Dictionary with models as keys and another dictionary as values, \n",
    "                where the inner dictionary has percentages as keys and p-values as values.\n",
    "    - output_path: Path to save the CSV file.\n",
    "    \"\"\"\n",
    "    # Create a DataFrame from the p_values dictionary\n",
    "    p_values_df = pd.DataFrame.from_dict({(model, percentage): p_value for model, percentages in p_values.items() for percentage, p_value in percentages.items()}, orient='index', columns=['P-value'])\n",
    "    \n",
    "    # Reset index to have model and percentage as columns\n",
    "    p_values_df.reset_index(inplace=True)\n",
    "    \n",
    "    # Rename columns\n",
    "    p_values_df.columns = ['Model_Percentage', 'P-value']\n",
    "    \n",
    "    # Split the 'Model_Percentage' column into 'Model' and 'Percentage'\n",
    "    p_values_df[['Model', 'Percentage']] = pd.DataFrame(p_values_df['Model_Percentage'].tolist(), index=p_values_df.index)\n",
    "    \n",
    "    # Drop the 'Model_Percentage' column\n",
    "    p_values_df.drop(columns=['Model_Percentage'], inplace=True)\n",
    "    \n",
    "    # Reorder columns\n",
    "    p_values_df = p_values_df[['Model', 'Percentage', 'P-value']]\n",
    "    \n",
    "    # Save to CSV\n",
    "    p_values_df.to_csv(output_path, index=False)\n",
    "\n",
    "\n",
    "# insert the path to the results\n",
    "AIxChem = Path.cwd().parent\n",
    "results = AIxChem / \"docs/buttar_norrby_results/holdout_RMSE\"\n",
    "\n",
    "# scoring metric\n",
    "metric = \"holdout_RMSE\"\n",
    "# fraction Plateau\n",
    "fraction_plateau = {\"RF\": \"90%\"}\n",
    "# number of folds\n",
    "n_folds = 20\n",
    "\n",
    "best_performances = get_best_augmented_performance(results_path=results, metric=metric)\n",
    "\n",
    "std_noaug = get_std_unaugmented_performances(mean_path=results, metric=metric, best_percentage_per_model=fraction_plateau)\n",
    "\n",
    "noaug_per_fold, aug_per_fold = get_performance_per_fold(folds_path=results, metric=metric, best_performances=best_performances, n_folds=n_folds, best_percentage_per_model=fraction_plateau)\n",
    "\n",
    "p_values, pv1, pv2 = perform_tost(noaug_per_fold=noaug_per_fold, aug_per_fold=aug_per_fold, metric=metric, standard_deviation=std_noaug)\n",
    "                                                \n",
    "plot_p_values(p_values=p_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
